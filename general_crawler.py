# ä½œè€… ED, Telegram è”ç»œ https://t.me/hongkongisp
#æœåŠ¡å™¨èµåŠ© é¡ºå®‰äº‘ https://say.cc
#çº¿ä¸Šå³æ—¶å¼€é€šäº‘ä¸»æœº, è¯·åˆ°é¡ºå®‰äº‘
import re
import argparse
import socket
import logging
import time
import requests
from urllib.parse import urlparse, urljoin, parse_qs, urlencode, urlunparse, unquote
from datetime import datetime
from http.cookiejar import LWPCookieJar, Cookie
import warnings
import os
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥colorlogï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ™®é€šæ—¥å¿—
try:
    import colorlog
    has_colorlog = True
except ImportError:
    has_colorlog = False

# å¿½ç•¥SSLè¯ä¹¦éªŒè¯çš„è­¦å‘Š
warnings.filterwarnings("ignore", message="Unverified HTTPS request")

# é…ç½®æ—¥å¿—ï¼ˆæ”¯æŒé¢œè‰²è¾“å‡ºï¼‰
def setup_logger(verbose):
    log_level = logging.DEBUG if verbose else logging.INFO
    
    if has_colorlog:
        log_format = (
            '%(asctime)s - '
            '%(log_color)s%(levelname)s%(reset)s - '
            '%(message)s'
        )
        color_formatter = colorlog.ColoredFormatter(
            log_format,
            datefmt='%Y-%m-%d %H:%M:%S',
            log_colors={
                'DEBUG': 'cyan',
                'INFO': 'green',
                'WARNING': 'yellow',
                'ERROR': 'red',
                'CRITICAL': 'red,bg_white',
            },
            secondary_log_colors={},
            style='%'
        )
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(color_formatter)
        file_handler = logging.FileHandler('general_crawler.log')
        file_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        
        logger = logging.getLogger(__name__)
        logger.setLevel(log_level)
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        return logger
    else:
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler('general_crawler.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)

# é€šç”¨è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Accept-Encoding": "gzip, deflate, br"
}

# åˆ›å»ºä¼šè¯ï¼Œä¿æŒcookies
session = requests.Session()
session.headers.update(HEADERS)
session.cookies = LWPCookieJar()
session.verify = False  # å…¨å±€ç¦ç”¨SSLè¯ä¹¦éªŒè¯

class FrameHandler:
    """å¤„ç†iframe/frameæ¡†æ¶å†…å®¹"""
    @staticmethod
    def extract_frame_sources(page_source, base_url, logger):
        frame_sources = []
        try:
            soup = BeautifulSoup(page_source, 'html.parser')
            for tag in ['iframe', 'frame']:
                elements = soup.find_all(tag)
                for elem in elements:
                    src = elem.get('src')
                    if src:
                        full_url = urljoin(base_url, src)
                        frame_sources.append(full_url)
                        logger.debug(f"å‘ç°{tag}æ¡†æ¶: {full_url}")
        except Exception as e:
            logger.warning(f"æå–æ¡†æ¶å†…å®¹æ—¶å‡ºé”™: {str(e)}")
        return frame_sources
    
    @staticmethod
    def fetch_frame_content(url, session, logger, timeout=10):
        # æ£€æŸ¥æ˜¯å¦ä¸ºPDFæ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
        if FrameHandler.is_pdf_url(url, logger):
            return None
            
        try:
            response = session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦ä¸ºPDF
            content_type = response.headers.get('Content-Type', '').lower()
            if 'application/pdf' in content_type:
                logger.debug(f"è·³è¿‡PDFå†…å®¹: {url}")
                return None
                
            logger.debug(f"æˆåŠŸè·å–æ¡†æ¶å†…å®¹: {url}")
            return response.text
        except Exception as e:
            logger.warning(f"è·å–æ¡†æ¶å†…å®¹å¤±è´¥ {url}: {str(e)}")
            return None
    
    @staticmethod
    def is_pdf_url(url, logger):
        """æ£€æŸ¥URLæ˜¯å¦æŒ‡å‘PDFæ–‡ä»¶"""
        # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦ä»¥.pdfç»“å°¾
        parsed = urlparse(url)
        if parsed.path.lower().endswith('.pdf'):
            logger.debug(f"æ£€æµ‹åˆ°PDF URL: {url}")
            return True
        return False

class DynamicContentHandler:
    """æå–JavaScriptåŠ¨æ€ç”Ÿæˆçš„é“¾æ¥"""
    @staticmethod
    def extract_dynamic_links(page_source, base_url, logger):
        dynamic_links = []
        patterns = [
            r'window\.location(?:\.href)?\s*=\s*["\']([^"\']+)["\']',
            r'location(?:\.href)?\s*=\s*["\']([^"\']+)["\']',
            r'window\.open\s*\(["\']([^"\']+)["\']',
            r'(?:redirect|goTo|loadPage)\s*\(["\']([^"\']+)["\']'
        ]
        for pattern in patterns:
            matches = re.findall(pattern, page_source, re.IGNORECASE)
            for match in matches:
                # è§£ç URLç¼–ç å­—ç¬¦
                decoded_match = unquote(match)
                full_url = urljoin(base_url, decoded_match)
                # æ£€æŸ¥æ˜¯å¦ä¸ºPDFé“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
                if not FrameHandler.is_pdf_url(full_url, logger):
                    dynamic_links.append(full_url)
                    logger.debug(f"å‘ç°åŠ¨æ€é“¾æ¥: {full_url}")
        return list(set(dynamic_links))

def force_load_cookies(cookie_file, domain, logger):
    """å¼ºåˆ¶åŠ è½½Cookieæ–‡ä»¶"""
    try:
        with open(cookie_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        cookies_added = 0
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            fields = re.split(r'[\t\s]+', line)
            if len(fields) != 7:
                logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {line_num}ï¼šå­—æ®µæ•°é‡ä¸æ­£ç¡®ï¼ˆ{len(fields)}ä¸ªï¼Œéœ€7ä¸ªï¼‰")
                continue
            
            domain_cookie, flag, path, secure, expiry, name, value = fields
            if not (domain_cookie == domain or 
                    domain_cookie.startswith(f'.{domain}') or
                    domain.endswith(domain_cookie)):
                continue
            
            try:
                expiry = int(expiry) if expiry else None
                secure = secure.upper() == 'TRUE'
                domain_specified = domain_cookie.startswith('.')
                
                cookie = Cookie(
                    version=0, name=name, value=value, port=None, port_specified=False,
                    domain=domain_cookie, domain_specified=domain_specified,
                    domain_initial_dot=domain_cookie.startswith('.'),
                    path=path, path_specified=True, secure=secure, expires=expiry,
                    discard=False, comment=None, comment_url=None, rest={}, rfc2109=False
                )
                session.cookies.set_cookie(cookie)
                cookies_added += 1
                logger.debug(f"å¼ºåˆ¶åŠ è½½Cookie: {name}={value}ï¼ˆåŸŸå: {domain_cookie}ï¼‰")
            except Exception as e:
                logger.warning(f"è§£æè¡Œ {line_num} å¤±è´¥: {str(e)}")
                continue
        
        logger.info(f"âœ… å¼ºåˆ¶åŠ è½½å®Œæˆï¼Œå…±æ·»åŠ  {cookies_added} ä¸ªCookie")
        return cookies_added > 0
    except Exception as e:
        logger.error(f"âŒ å¼ºåˆ¶åŠ è½½Cookieå¤±è´¥: {str(e)}")
        return False

def save_cookies_to_file(cookies, url, logger, suffix=""):
    """ä¿å­˜Cookieåˆ°cookieæ–‡ä»¶å¤¹"""
    try:
        if not os.path.exists('cookie'):
            os.makedirs('cookie')
            logger.debug("åˆ›å»ºcookieæ–‡ä»¶å¤¹")
        
        parsed_url = urlparse(url)
        safe_domain = re.sub(r'[^\w\-_.]', '_', parsed_url.netloc)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        filename = f"cookie/{safe_domain}_{timestamp}_{suffix}.txt" if suffix else f"cookie/{safe_domain}_{timestamp}.txt"
        
        content = [
            "# Netscape HTTP Cookie File\n",
            "# https://curl.se/docs/http-cookies.html\n",
            f"# æ¥æºURL: {url}\n",
            f"# ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            "\n"
        ]
        for cookie in cookies:
            domain = cookie.domain
            flag = "TRUE" if cookie.domain_specified else "FALSE"
            path = cookie.path
            secure = "TRUE" if cookie.secure else "FALSE"
            expiry = cookie.expires if cookie.expires else 0
            line = f"{domain}\t{flag}\t{path}\t{secure}\t{expiry}\t{cookie.name}\t{cookie.value}\n"
            content.append(line)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.writelines(content)
        
        logger.debug(f"å·²ä¿å­˜ {len(cookies)} ä¸ªCookieåˆ° {filename}")
        return filename
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜Cookieåˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

class CookieFileFixer:
    """ä¿®å¤Cookieæ–‡ä»¶æ ¼å¼"""
    @staticmethod
    def fix_cookie_file(input_path, output_path=None, logger=None):
        if not output_path:
            base, ext = os.path.splitext(input_path)
            output_path = f"{base}_fixed{ext}"
        
        try:
            with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            has_netscape_header = any(
                line.strip().lower().startswith('# netscape http cookie file') 
                for line in lines
            )
            fixed_lines = []
            
            if not has_netscape_header:
                fixed_lines.extend([
                    "# Netscape HTTP Cookie File\n",
                    "# https://curl.se/docs/http-cookies.html\n",
                    "# Fixed by General Crawler\n",
                    "\n"
                ])
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if ' ' in line and '\t' not in line:
                    fields = line.split()
                    if len(fields) == 7:
                        fixed_line = '\t'.join(fields) + '\n'
                        fixed_lines.append(fixed_line)
                        logger.debug(f"ä¿®å¤è¡Œ {line_num}ï¼šç©ºæ ¼è½¬Tab")
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {line_num}ï¼šå­—æ®µæ•°{len(fields)}ï¼ˆéœ€7ä¸ªï¼‰")
                elif '\t' in line:
                    fields = line.split('\t')
                    if len(fields) == 7:
                        if fields[1].lower() in ['true', 'false']:
                            fields[1] = fields[1].upper()
                        if fields[3].lower() in ['true', 'false']:
                            fields[3] = fields[3].upper()
                        fixed_lines.append('\t'.join(fields) + '\n')
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {line_num}ï¼šå­—æ®µæ•°{len(fields)}ï¼ˆéœ€7ä¸ªï¼‰")
                else:
                    logger.warning(f"è·³è¿‡æ— æ³•è¯†åˆ«çš„è¡Œ {line_num}")
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.writelines(fixed_lines)
            logger.info(f"âœ… Cookieæ–‡ä»¶ä¿®å¤å®Œæˆï¼š{output_path}")
            return output_path
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤Cookieæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

class GeneralCrawler:
    def __init__(self, target_url, delay_ms=2000, max_depth=3, max_pages=50, 
                 ignore_ssl=False, cookie_file=None, cookie_mode='all', 
                 save_cookies_frequency=10, logger=None, allow_cross_domain=False,
                 timeout_ms=2000):  # æ–°å¢è¶…æ—¶å‚æ•°
        # è¿‡æ»¤è§„åˆ™ - åŒ…å«PDFæ–‡ä»¶è¿‡æ»¤
        self.exclude_extensions = r'\.(pdf|jpg|jpeg|png|gif|bmp|tiff|webp|ico|doc|docx|xls|xlsx|ppt|pptx|zip|rar|tar|gz|7z|exe|dll|bin|iso|mp3|mp4|avi|mov|flv|wmv)$'
        self.exclude_prefixes = r'^(javascript:|data:|tel:|sms:|ftp:|irc:|magnet:|mailto:)'
        # é€šç”¨GETå‚æ•°é›†åˆ
        self.keep_params = {'id', 'cate', 'fid', 'gid', 'action', 'q', 'query', 'page', 'p', 
                           'wd', 'lang', 'hl', 'offset', 'limit', 'sort', 'order'}
        self.login_paths = r'(/login|/signin|/account/login|/user/login|/auth/login|/session/new)'
        
        # Cookieæ£€æµ‹å‚æ•°
        self.cookie_check_params = {'cookie', 'cookies', 'ck', 'cookie_support', 'cookie_check', 'session', 'sid'}
        
        # å†…å®¹é¡µé¢æ¨¡å¼
        self.content_page_patterns = [
            r'/detail\.html\?id=\d+',
            r'/view\.php\?id=\d+',
            r'/article/\d+',
            r'/news/\d+',
            r'/post/\d+'
        ]
        
        # æ¡†æ¶å’ŒåŠ¨æ€å†…å®¹å¤„ç†å¼€å…³
        self.handle_frames = True
        self.handle_dynamic_content = True
        
        # è°ƒè¯•å’Œæ€§èƒ½é…ç½®
        self.max_links_per_page = 5000  # æ¯é¡µæœ€å¤§å¤„ç†é“¾æ¥æ•°
        self.link_queue_warning_threshold = 10000  # é˜Ÿåˆ—è­¦å‘Šé˜ˆå€¼
        
        # Cookieå¤„ç†é…ç½®
        self.cookie_mode = cookie_mode
        self.login_cookie_keywords = {'session', 'token', 'user', 'auth', 'login', 'sid', 'uid', 'sessionid'}
        self.browsing_cookie_keywords = {'theme', 'lang', 'view', 'layout', 'font', 'preference', 'cookie_accepted'}
        self.save_cookies_frequency = save_cookies_frequency  # Cookieä¿å­˜é¢‘ç‡
        self.last_saved_cookie_count = 0  # ä¸Šæ¬¡ä¿å­˜çš„Cookieæ•°é‡
        self.saved_cookie_files = []  # ä¿å­˜çš„Cookieæ–‡ä»¶åˆ—è¡¨
        
        # è¶…æ—¶è®¾ç½® - æ–°å¢
        self.timeout = timeout_ms / 1000.0  # è½¬æ¢ä¸ºç§’
        self.frame_timeout = max(5.0, self.timeout)  # æ¡†æ¶è¶…æ—¶è‡³å°‘5ç§’
        
        # ç›®æ ‡URLå¤„ç† - æ”¯æŒå¸¦GETå‚æ•°çš„URL
        if not target_url.startswith(('http://', 'https://')):
            target_url = f'https://{target_url}'
        parsed_url = urlparse(target_url)
        self.base_domain = parsed_url.netloc
        self.start_url = target_url
        self.allow_cross_domain = allow_cross_domain  # æ˜¯å¦å…è®¸è·¨åŸŸçˆ¬å–
        
        # åŠ è½½Cookieæ–‡ä»¶
        self.cookie_file = cookie_file
        self.fixed_cookie_file = None
        if self.cookie_file and logger:
            load_success = self.load_cookies(self.cookie_file, logger)
            if not load_success:
                logger.info("å°è¯•è‡ªåŠ¨ä¿®å¤Cookieæ–‡ä»¶...")
                self.fixed_cookie_file = CookieFileFixer.fix_cookie_file(self.cookie_file, logger=logger)
                if self.fixed_cookie_file:
                    load_success = self.load_cookies(self.fixed_cookie_file, logger, is_fixed=True)
            if not load_success:
                logger.info("æ ‡å‡†åŠ è½½å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åŠ è½½Cookie...")
                force_load_cookies(self.cookie_file, self.base_domain, logger)
                if self.fixed_cookie_file:
                    force_load_cookies(self.fixed_cookie_file, self.base_domain, logger)
        
        # å­˜å‚¨ç»“æ„ - é‚®ç®±å»é‡æœºåˆ¶ï¼šä½¿ç”¨å°å†™é‚®ç®±ä½œä¸ºé”®
        self.visited_urls = set()
        self.processed_frames = set()
        self.queue = [(self._standardize_url(target_url, keep_all_get_params=True), 0)]
        self.emails = {}  # é”®ï¼šå°å†™é‚®ç®±ï¼Œå€¼ï¼šåŸå§‹é‚®ç®±ï¼ˆä¿ç•™åŸå§‹å¤§å°å†™ï¼‰
        self.blocked_urls = {"login": set(), "captcha": set()}
        self.pdf_urls = set()  # è®°å½•æ£€æµ‹åˆ°çš„PDFæ–‡ä»¶URL
        
        # æ§åˆ¶å‚æ•°
        self.delay = delay_ms / 1000.0
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.crawled_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯ - åŒ…å«é‚®ç®±å»é‡å’Œè¶…æ—¶ç»Ÿè®¡
        self.status_stats = {"200":0,"404":0,"403":0,"è¶…æ—¶":0,"å…¶ä»–é”™è¯¯":0,"è¢«loginæ‹¦æˆª":0,"è¢«captchaæ‹¦æˆª":0}
        self.depth_stats = {}
        self.speed_stats = []
        self.frame_stats = {"found":0,"processed":0,"failed":0, "è¶…æ—¶":0}  # å¢åŠ æ¡†æ¶è¶…æ—¶ç»Ÿè®¡
        self.cookie_stats = {"total":0,"login":0,"browsing":0,"unknown":0}
        self.link_stats = {"extracted":0, "added":0, "duplicates":0, "filtered":0, "pdf":0}
        self.email_stats = {"found":0, "unique":0, "duplicates":0}  # é‚®ç®±ç»Ÿè®¡
        
        # è¾“å‡ºæ–‡ä»¶
        safe_domain = self.base_domain.replace('.', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_file = f"emails_{safe_domain}_{timestamp}.txt"
        self.blocked_file = f"blocked_urls_{safe_domain}_{timestamp}.txt"
        self.pdf_file = f"pdf_urls_{safe_domain}_{timestamp}.txt"  # ä¿å­˜æ£€æµ‹åˆ°çš„PDF URL
        
        self.logger = logger or logging.getLogger(__name__)

    def _classify_cookie(self, cookie_name):
        """åˆ†ç±»Cookieç±»å‹"""
        cookie_name_lower = cookie_name.lower()
        for keyword in self.login_cookie_keywords:
            if keyword in cookie_name_lower:
                return "login"
        for keyword in self.browsing_cookie_keywords:
            if keyword in cookie_name_lower:
                return "browsing"
        return "unknown"

    def load_cookies(self, cookie_file, logger, is_fixed=False):
        """åŠ è½½Cookieæ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ¨¡å¼è¿‡æ»¤æ‰€éœ€ç±»å‹çš„Cookie"""
        try:
            self.cookie_stats = {"total": 0, "login": 0, "browsing": 0, "unknown": 0}
            
            if not os.path.exists(cookie_file):
                logger.warning(f"âš ï¸ Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}")
                return False
                
            if os.path.getsize(cookie_file) == 0:
                logger.warning(f"âš ï¸ Cookieæ–‡ä»¶ä¸ºç©º: {cookie_file}")
                return False

            temp_cj = LWPCookieJar()
            temp_cj.load(cookie_file, ignore_discard=True, ignore_expires=True)
            
            loaded_count = 0
            for cookie in temp_cj:
                if not (cookie.domain == self.base_domain or 
                        cookie.domain.startswith(f'.{self.base_domain}') or
                        self.base_domain.endswith(cookie.domain)):
                    continue
                
                cookie_type = self._classify_cookie(cookie.name)
                self.cookie_stats["total"] += 1
                self.cookie_stats[cookie_type] += 1
                
                if self.cookie_mode == 'all':
                    session.cookies.set_cookie(cookie)
                    loaded_count += 1
                elif self.cookie_mode == 'login-only' and cookie_type == 'login':
                    session.cookies.set_cookie(cookie)
                    loaded_count += 1
                elif self.cookie_mode == 'browsing-only' and cookie_type == 'browsing':
                    session.cookies.set_cookie(cookie)
                    loaded_count += 1
            
            logger.info(
                f"ğŸª CookieåŠ è½½ç»Ÿè®¡: æ€»è®¡{self.cookie_stats['total']}ä¸ª "
                f"(ç™»å½•ç›¸å…³: {self.cookie_stats['login']}, æµè§ˆè®¾ç½®: {self.cookie_stats['browsing']})"
            )
            logger.info(f"ğŸ“Œ æ ¹æ®æ¨¡å¼ '{self.cookie_mode}' åŠ è½½äº† {loaded_count} ä¸ªCookie")
            
            if is_fixed:
                logger.info("âœ… ä¿®å¤åçš„Cookieæ–‡ä»¶åŠ è½½æˆåŠŸï¼")
                
            # ä¿å­˜åˆå§‹åŠ è½½çš„Cookie
            if loaded_count > 0:
                cookie_file = save_cookies_to_file(
                    session.cookies, 
                    self.start_url, 
                    self.logger, 
                    suffix="initial"
                )
                if cookie_file:
                    self.saved_cookie_files.append(cookie_file)
                
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½Cookieæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def _standardize_url(self, url, keep_all_get_params=False):
        """æ ‡å‡†åŒ–URLï¼šä¿ç•™GETå‚æ•°ï¼Œä»…å»é”šç‚¹"""
        parsed = urlparse(url)
        # ç§»é™¤é”šç‚¹ï¼Œä½†ä¿ç•™æ‰€æœ‰GETå‚æ•°
        parsed = parsed._replace(fragment='')
        
        # å¦‚æœæ˜¯èµ·å§‹URLæˆ–éœ€è¦ä¿ç•™æ‰€æœ‰GETå‚æ•°ï¼Œåˆ™ä¸è¿‡æ»¤å‚æ•°
        if not keep_all_get_params and parsed.query:
            query_dict = parse_qs(parsed.query)
            filtered_params = {}
            
            for param, values in query_dict.items():
                # ä¿ç•™é‡è¦å‚æ•°
                if param in self.keep_params:
                    valid_values = [v.strip() for v in values if v.strip()]
                    if valid_values:
                        filtered_params[param] = valid_values[0]
                elif param in self.cookie_check_params:
                    valid_values = [v.strip() for v in values if v.strip()]
                    if valid_values:
                        filtered_params[param] = valid_values[0]
        
            sorted_params = sorted(filtered_params.items())
            parsed = parsed._replace(query=urlencode(sorted_params))
        
        # è§£ç URLä»¥ç¡®ä¿ä¸€è‡´æ€§
        standardized = urlunparse(parsed)
        return unquote(standardized)

    def _should_exclude_link(self, link):
        """è¿‡æ»¤æ— æ•ˆé“¾æ¥ï¼ŒåŒ…å«PDFæ–‡ä»¶è¿‡æ»¤"""
        if not link or link.strip() == '':
            self.link_stats["filtered"] += 1
            return True
        
        parsed = urlparse(link)
        
        # ç‰¹åˆ«æ£€æŸ¥æ˜¯å¦ä¸ºPDFæ–‡ä»¶
        if parsed.path.lower().endswith('.pdf'):
            self.logger.debug(f"ğŸ” è¿‡æ»¤PDFæ–‡ä»¶: {link}")
            self.link_stats["filtered"] += 1
            self.link_stats["pdf"] += 1
            self.pdf_urls.add(link)  # è®°å½•PDF URL
            return True
        
        # è¿‡æ»¤ç™»å½•ç›¸å…³è·¯å¾„
        if re.search(self.login_paths, parsed.path, re.IGNORECASE):
            self.logger.debug(f"ğŸ”’ è¿‡æ»¤ç™»å½•ç›¸å…³è·¯å¾„: {link}")
            self.link_stats["filtered"] += 1
            return True
        
        if re.match(self.exclude_prefixes, link, re.IGNORECASE):
            self.link_stats["filtered"] += 1
            return True
        
        # è¿‡æ»¤åª’ä½“æ–‡ä»¶
        media_extensions = r'\.(jpg|jpeg|png|gif|bmp|tiff|webp|ico|mp3|mp4|avi|mov|flv|wmv)$'
        if re.search(media_extensions, link, re.IGNORECASE):
            self.link_stats["filtered"] += 1
            return True
        
        # å¤„ç†åŸŸåé™åˆ¶
        if parsed.netloc and not self.allow_cross_domain:
            if not (parsed.netloc == self.base_domain or parsed.netloc.endswith(f'.{self.base_domain}')):
                self.link_stats["filtered"] += 1
                return True
        
        return False

    def _is_blocked_page(self, page_source, url):
        """é¡µé¢æ£€æµ‹é€»è¾‘"""
        parsed_url = urlparse(url)
        path = parsed_url.path or '/'
        
        # å†…å®¹é¡µé¢æ¨¡å¼åŒ¹é…
        for pattern in self.content_page_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                self.logger.debug(f"ğŸ“Œ å†…å®¹é¡µé¢æ¨¡å¼åŒ¹é…: {url} ä¸åˆ¤å®šä¸ºç™»å½•é¡µ")
                return False, None
        
        # æ£€æŸ¥URLæ˜¯å¦åŒ…å«å†…å®¹IDå‚æ•°
        query_params = parse_qs(parsed_url.query)
        if 'id' in query_params and query_params['id'][0].isdigit():
            self.logger.debug(f"ğŸ“Œ åŒ…å«å†…å®¹IDå‚æ•°ï¼Œä¸åˆ¤å®šä¸ºç™»å½•é¡µ: {url}")
            return False, None
        
        # æ£€æŸ¥URLæ˜¯å¦åŒ…å«Cookieå‚æ•°
        has_cookie_param = any(param in self.cookie_check_params for param in query_params.keys())
        if has_cookie_param:
            self.logger.debug(f"ğŸª åŒ…å«Cookieå‚æ•°ï¼Œä¸åˆ¤å®šä¸ºç™»å½•é¡µ: {url}")
            return False, None
        
        # è§£æé¡µé¢ç»“æ„
        soup = BeautifulSoup(page_source, 'html.parser')
        page_text = page_source.lower()
        
        # ç™»å½•é¡µé¢æ£€æµ‹
        login_keywords = ['ç™»å½•', 'æ³¨å†Œ', 'sign in', 'login', 'sign up', 'è¯·ç™»å½•', 'ä¼šå‘˜ä¸­å¿ƒ']
        keyword_count = sum(1 for kw in login_keywords if kw.lower() in page_text)
        
        # è¡¨å•å…ƒç´ æ£€æµ‹
        password_fields = soup.find_all('input', {'type': 'password'})
        
        if keyword_count >= 2 and len(password_fields) > 0:
            self.logger.warning(f"ğŸ”’ æ£€æµ‹åˆ°ç™»å½•é¡µé¢: {url}")
            return True, "login"
        
        # éªŒè¯ç é¡µé¢æ£€æµ‹
        captcha_keywords = ['éªŒè¯ç ', 'captcha', 'å›¾å½¢éªŒè¯', 'security code', 'å®‰å…¨éªŒè¯']
        has_captcha_keywords = any(kw.lower() in page_text for kw in captcha_keywords)
        
        captcha_elements = (soup.find_all('img', alt=re.compile('|'.join(captcha_keywords), re.IGNORECASE)) or
                          soup.find_all('input', {'name': re.compile('captcha', re.IGNORECASE)}))
        
        if has_captcha_keywords and len(captcha_elements) > 0:
            self.logger.warning(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç é¡µé¢: {url}")
            return True, "captcha"
        
        return False, None

    def _process_frames(self, current_url, page_source, current_depth):
        """å¤„ç†é¡µé¢ä¸­çš„æ¡†æ¶å†…å®¹ï¼Œä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´"""
        if not self.handle_frames or current_depth >= self.max_depth:
            return ""
            
        frame_urls = FrameHandler.extract_frame_sources(page_source, current_url, self.logger)
        self.frame_stats["found"] += len(frame_urls)
        
        combined_content = ""
        
        for frame_url in frame_urls:
            # æ£€æŸ¥æ˜¯å¦ä¸ºPDFæ–‡ä»¶
            if FrameHandler.is_pdf_url(frame_url, self.logger):
                self.link_stats["pdf"] += 1
                self.pdf_urls.add(frame_url)
                self.logger.debug(f"è·³è¿‡PDFæ¡†æ¶: {frame_url}")
                continue
                
            standardized_url = self._standardize_url(frame_url)
            
            if standardized_url in self.processed_frames:
                continue
                
            if self._should_exclude_link(standardized_url):
                continue
                
            self.processed_frames.add(standardized_url)
            
            # ä½¿ç”¨æ¡†æ¶è¶…æ—¶æ—¶é—´ï¼ˆè‡³å°‘5ç§’ï¼‰
            frame_content = FrameHandler.fetch_frame_content(
                standardized_url, 
                session, 
                self.logger, 
                timeout=self.frame_timeout  # ä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶
            )
            
            if frame_content:
                self.frame_stats["processed"] += 1
                combined_content += frame_content + "\n\n"
                self._extract_links(standardized_url, frame_content, current_depth)
                self._extract_emails(frame_content)
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶å¯¼è‡´çš„å¤±è´¥
                if "è¶…æ—¶" in str(frame_content).lower():
                    self.frame_stats["è¶…æ—¶"] += 1
                self.frame_stats["failed"] += 1
        
        return combined_content

    def _save_current_cookies(self, current_url):
        """ä¿å­˜å½“å‰ä¼šè¯ä¸­çš„Cookie"""
        current_cookie_count = len(session.cookies)
        
        # ä»…åœ¨Cookieæœ‰å˜åŒ–æ—¶æ‰ä¿å­˜
        if current_cookie_count != self.last_saved_cookie_count:
            cookie_file = save_cookies_to_file(
                session.cookies, 
                current_url, 
                self.logger,
                suffix=f"page{self.crawled_count}"
            )
            if cookie_file:
                self.saved_cookie_files.append(cookie_file)
                self.last_saved_cookie_count = current_cookie_count
                self.logger.info(f"ğŸ’¾ å·²ä¿å­˜Cookieåˆ° {cookie_file} (å…± {current_cookie_count} ä¸ª)")
        else:
            self.logger.debug(f"Cookieæœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡ä¿å­˜ (å½“å‰ {current_cookie_count} ä¸ª)")

    def _extract_links(self, current_url, page_source, current_depth):
        """ä»é¡µé¢æºç æå–é“¾æ¥ï¼Œè·³è¿‡PDFé“¾æ¥"""
        if current_depth >= self.max_depth:
            self.logger.debug(f"ğŸ“‰ å·²è¾¾æœ€å¤§æ·±åº¦ {self.max_depth}ï¼Œä¸å†æå–æ–°é“¾æ¥")
            return
        
        # æå–aæ ‡ç­¾é“¾æ¥
        link_patterns = [r'<a [^>]*href=["\']([^"\']+)["\']']
        matches = []
        for pattern in link_patterns:
            matches.extend(re.findall(pattern, page_source, re.IGNORECASE))
        
        # æå–åŠ¨æ€é“¾æ¥ï¼ˆå·²åœ¨DynamicContentHandlerä¸­è¿‡æ»¤PDFï¼‰
        if self.handle_dynamic_content:
            dynamic_matches = DynamicContentHandler.extract_dynamic_links(page_source, current_url, self.logger)
            matches.extend(dynamic_matches)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        unique_links = list(set(matches))
        self.link_stats["extracted"] += len(unique_links)
        
        # å¦‚æœé“¾æ¥è¿‡å¤šï¼Œè¿›è¡Œæˆªæ–­å¹¶è®°å½•è­¦å‘Š
        if len(unique_links) > self.max_links_per_page:
            self.logger.warning(f"âš ï¸ é¡µé¢é“¾æ¥è¿‡å¤šï¼Œæˆªæ–­ä¸º {self.max_links_per_page} ä¸ªï¼ˆåŸå§‹æ•°é‡: {len(unique_links)}ï¼‰")
            unique_links = unique_links[:self.max_links_per_page]
        
        self.logger.debug(f"ä» {current_url} æå–åˆ° {len(unique_links)} ä¸ªé“¾æ¥ (ç´¯è®¡æå–: {self.link_stats['extracted']})")
        
        new_links_added = 0
        for raw_link in unique_links:
            raw_link = raw_link.strip().replace('\\', '')
            # è§£ç URLç¼–ç å­—ç¬¦
            decoded_link = unquote(raw_link)
            full_link = urljoin(current_url, decoded_link)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºPDFé“¾æ¥
            if full_link.lower().endswith('.pdf'):
                self.link_stats["pdf"] += 1
                self.pdf_urls.add(full_link)
                self.logger.debug(f"è·³è¿‡PDFé“¾æ¥: {full_link}")
                continue
                
            standardized_link = self._standardize_url(full_link)
            
            if self._should_exclude_link(standardized_link):
                continue
            
            new_depth = current_depth + 1
            
            if new_depth > self.max_depth:
                self.logger.debug(f"ğŸ“‰ è¶…æ·±åº¦é™åˆ¶ï¼ˆ{new_depth} > {self.max_depth}ï¼‰ï¼Œè·³è¿‡: {standardized_link}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®æˆ–å·²åœ¨é˜Ÿåˆ—ä¸­
            already_visited = standardized_link in self.visited_urls
            already_in_queue = any(standardized_link == q[0] for q in self.queue)
            
            if not already_visited and not already_in_queue:
                self.queue.append((standardized_link, new_depth))
                new_links_added += 1
                self.link_stats["added"] += 1
            else:
                self.link_stats["duplicates"] += 1
        
        self.logger.debug(f"ä» {current_url} å‘é˜Ÿåˆ—æ·»åŠ äº† {new_links_added} ä¸ªæ–°é“¾æ¥ (ç´¯è®¡æ·»åŠ : {self.link_stats['added']})")

    def _extract_emails(self, page_source):
        """æå–é‚®ç®±åœ°å€ï¼Œå¤„ç†å¤§å°å†™ä¸åŒçš„é‡å¤é‚®ç®±"""
        email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}'
        found_emails = re.findall(email_pattern, page_source)
        
        for email in found_emails:
            # è½¬æ¢ä¸ºå°å†™ä½œä¸ºé”®ï¼Œç”¨äºæ£€æµ‹é‡å¤
            email_lower = email.lower()
            self.email_stats["found"] += 1  # ç´¯è®¡æ€»å‘ç°æ•°
            
            if email_lower not in self.emails:
                # æ–°é‚®ç®±ï¼Œæ·»åŠ åˆ°å­—å…¸
                self.emails[email_lower] = email
                self.email_stats["unique"] += 1  # ç´¯è®¡å”¯ä¸€æ•°
                self.logger.info(f"ğŸ“§ å‘ç°æ–°é‚®ç®±: {email}ï¼ˆæ€»è®¡å”¯ä¸€: {self.email_stats['unique']}ï¼‰")
            else:
                # é‡å¤é‚®ç®±ï¼Œä»…æ›´æ–°ç»Ÿè®¡
                self.email_stats["duplicates"] += 1  # ç´¯è®¡é‡å¤æ•°
                self.logger.debug(f"ğŸ”„ å‘ç°é‡å¤é‚®ç®±: {email}ï¼ˆåŸå§‹: {self.emails[email_lower]}ï¼‰")

    def _get_url_ip(self, url):
        """è·å–URLå¯¹åº”çš„IP"""
        try:
            parsed = urlparse(url)
            return socket.gethostbyname(parsed.netloc)
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ— æ³•è§£æ {url} çš„IPåœ°å€: {str(e)[:30]}")
            return "æœªçŸ¥"

    def _update_status_stats(self, status):
        """æ›´æ–°çŠ¶æ€ç»Ÿè®¡"""
        if status in self.status_stats:
            self.status_stats[status] += 1
        else:
            self.status_stats["å…¶ä»–é”™è¯¯"] += 1

    def _save_pdf_urls(self):
        """ä¿å­˜æ£€æµ‹åˆ°çš„PDFæ–‡ä»¶URL"""
        if not self.pdf_urls:
            return
            
        try:
            with open(self.pdf_file, 'w', encoding='utf-8') as f:
                f.write("=== æ£€æµ‹åˆ°çš„PDFæ–‡ä»¶URL ===\n")
                for url in sorted(self.pdf_urls):
                    f.write(f"{url}\n")
            
            self.logger.info(f"ğŸ“‹ å·²ä¿å­˜ {len(self.pdf_urls)} ä¸ªPDFæ–‡ä»¶URLåˆ° {self.pdf_file}")
        except IOError as e:
            self.logger.error(f"âŒ ä¿å­˜PDF URLå¤±è´¥: {str(e)}")

    def crawl(self):
        """ä¸»çˆ¬å–é€»è¾‘ï¼Œä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´"""
        self.logger.info("="*50)
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ç›®æ ‡: {self.base_domain}")
        self.logger.info(f"ğŸ¯ èµ·å§‹URL: {self.start_url}")
        self.logger.info(f"âš™ï¸  é…ç½®å‚æ•°: é—´éš”={self.delay*1000:.0f}ms | è¶…æ—¶={self.timeout*1000:.0f}ms | æœ€å¤§æ·±åº¦={self.max_depth} | æœ€å¤§é¡µé¢={self.max_pages}")
        self.logger.info(f"ğŸŒ è·¨åŸŸçˆ¬å–: {'å¯ç”¨' if self.allow_cross_domain else 'ç¦ç”¨'}")
        self.logger.info(f"ğŸ“„ è¿‡æ»¤è®¾ç½®: è‡ªåŠ¨è·³è¿‡PDFæ–‡ä»¶åŠå…¶ä»–åª’ä½“æ–‡ä»¶")
        self.logger.info(f"âœ‰ï¸  é‚®ç®±å¤„ç†: è‡ªåŠ¨å»é‡ï¼ˆå¤§å°å†™ä¸åŒè§†ä¸ºç›¸åŒé‚®ç®±ï¼‰")
        self.logger.info(f"ğŸª Cookieè®¾ç½®: æ¯çˆ¬å–{self.save_cookies_frequency}é¡µä¿å­˜ä¸€æ¬¡ | æ¨¡å¼={self.cookie_mode}")
        self.logger.info(f"ğŸ”§ æ¡†æ¶å¤„ç†: {'å¯ç”¨' if self.handle_frames else 'ç¦ç”¨'} | åŠ¨æ€å†…å®¹å¤„ç†: {'å¯ç”¨' if self.handle_dynamic_content else 'ç¦ç”¨'}")
        self.logger.info(f"ğŸª åˆå§‹Cookieæ•°é‡: {len(session.cookies)}ä¸ª")
        if self.cookie_file:
            self.logger.info(f"ğŸ“‚ ä½¿ç”¨åˆå§‹Cookieæ–‡ä»¶: {self.cookie_file}")
        self.logger.info(f"ğŸ“„ ç»“æœå°†ä¿å­˜åˆ°: {self.output_file}")
        self.logger.info("="*50)

        loop_counter = 0  # ç”¨äºæ§åˆ¶è°ƒè¯•ä¿¡æ¯è¾“å‡ºé¢‘ç‡
        while self.queue and self.crawled_count < self.max_pages:
            # æ¯10æ¬¡å¾ªç¯è¾“å‡ºä¸€æ¬¡é˜Ÿåˆ—çŠ¶æ€
            loop_counter += 1
            if loop_counter % 10 == 0:
                self.logger.debug(
                    f"ğŸ“Š é˜Ÿåˆ—çŠ¶æ€: å¾…å¤„ç†={len(self.queue)} | "
                    f"å·²è®¿é—®={len(self.visited_urls)} | "
                    f"å·²çˆ¬å–={self.crawled_count}/{self.max_pages} | "
                    f"é“¾æ¥ç»Ÿè®¡: æå–={self.link_stats['extracted']} | "
                    f"æ–°å¢={self.link_stats['added']} | "
                    f"é‡å¤={self.link_stats['duplicates']} | "
                    f"è¿‡æ»¤={self.link_stats['filtered']} | "
                    f"PDFè¿‡æ»¤={self.link_stats['pdf']} | "
                    f"é‚®ç®±ç»Ÿè®¡: å‘ç°={self.email_stats['found']} | å”¯ä¸€={self.email_stats['unique']} | é‡å¤={self.email_stats['duplicates']}"
                )
            
            # é˜Ÿåˆ—è¿‡å¤§æ—¶å‘å‡ºè­¦å‘Š
            if len(self.queue) > self.link_queue_warning_threshold:
                self.logger.warning(f"âš ï¸ é˜Ÿåˆ—è¿‡å¤§ ({len(self.queue)} ä¸ªé“¾æ¥)ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

            current_url, current_depth = self.queue.pop(0)
            
            # æ£€æŸ¥å½“å‰URLæ˜¯å¦ä¸ºPDFæ–‡ä»¶
            if current_url.lower().endswith('.pdf'):
                self.logger.debug(f"è·³è¿‡PDFæ–‡ä»¶çˆ¬å–: {current_url}")
                self.pdf_urls.add(current_url)
                self.link_stats["pdf"] += 1
                continue
                
            if current_depth > self.max_depth:
                self.logger.debug(f"ğŸ“‰ è·³è¿‡è¶…æ·±åº¦é¡µé¢ï¼ˆ{current_depth} > {self.max_depth}ï¼‰: {current_url}")
                continue
            
            if current_url in self.visited_urls:
                self.logger.debug(f"ğŸ”„ è·³è¿‡å·²è®¿é—®é¡µé¢: {current_url}")
                continue
            
            self.depth_stats[current_depth] = self.depth_stats.get(current_depth, 0) + 1
            
            self.visited_urls.add(current_url)
            self.crawled_count += 1
            url_ip = self._get_url_ip(current_url)
            status = "æœªçŸ¥"
            response_time = 0

            try:
                start_time = time.time()
                
                # ä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´
                response = session.get(
                    current_url,
                    timeout=self.timeout,  # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶
                    allow_redirects=True,
                    verify=False
                )
                
                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºPDFæ–‡ä»¶
                content_type = response.headers.get('Content-Type', '').lower()
                if 'application/pdf' in content_type:
                    self.logger.warning(f"æ£€æµ‹åˆ°PDFå†…å®¹ï¼Œè·³è¿‡å¤„ç†: {current_url}")
                    self.pdf_urls.add(current_url)
                    self.link_stats["pdf"] += 1
                    continue
                
                response_time = time.time() - start_time
                self.speed_stats.append(response_time)
                
                status = str(response.status_code)
                self._update_status_stats(status)
                
                cookie_count = len(session.cookies)
                self.logger.info(
                    f"ğŸ“„ çˆ¬å– [{self.crawled_count}/{self.max_pages}]: {current_url} "
                    f"(IP: {url_ip}, æ·±åº¦: {current_depth}, çŠ¶æ€: {status}, Cookie: {cookie_count}ä¸ª, è€—æ—¶: {response_time:.2f}ç§’)"
                )
                self.logger.debug(
                    f"ğŸ’¨ å“åº”é€Ÿåº¦: {response_time:.3f}ç§’ | å†…å®¹å¤§å°: {len(response.text)/1024:.1f}KB"
                )

                page_source = response.text
                frame_content = self._process_frames(current_url, page_source, current_depth)
                full_content = page_source + "\n\n" + frame_content
                
                is_blocked, block_type = self._is_blocked_page(full_content, current_url)
                if is_blocked:
                    self.blocked_urls[block_type].add(current_url)
                    self._update_status_stats(f"è¢«{block_type}æ‹¦æˆª")
                    self.logger.info(f"â±ï¸ è·³è¿‡{block_type}é¡µé¢: {current_url}")
                else:
                    self._extract_links(current_url, full_content, current_depth)
                    self._extract_emails(full_content)
                
                # æŒ‰é¢‘ç‡ä¿å­˜Cookie
                if self.crawled_count % self.save_cookies_frequency == 0:
                    self._save_current_cookies(current_url)

            except requests.exceptions.Timeout:
                status = "è¶…æ—¶"
                self._update_status_stats("è¶…æ—¶")
                self.logger.error(f"â±ï¸  çˆ¬å–è¶…æ—¶ [{self.crawled_count}]: {current_url}ï¼ˆIP: {url_ip}ï¼Œè¶…æ—¶é˜ˆå€¼: {self.timeout}ç§’ï¼‰")
            except requests.exceptions.SSLError:
                status = "SSLé”™è¯¯"
                self._update_status_stats("å…¶ä»–é”™è¯¯")
                self.logger.error(f"ğŸ”’ SSLè¯ä¹¦é”™è¯¯ [{self.crawled_count}]: {current_url}")
            except requests.exceptions.HTTPError as e:
                status = str(e.response.status_code) if e.response else "HTTPé”™è¯¯"
                self._update_status_stats(status)
                self.logger.error(f"âŒ HTTPé”™è¯¯ [{self.crawled_count}]: {str(e)[:50]}")
            except requests.exceptions.RequestException as e:
                status = "å…¶ä»–é”™è¯¯"
                self._update_status_stats("å…¶ä»–é”™è¯¯")
                self.logger.error(f"âŒ è¯·æ±‚å¼‚å¸¸ [{self.crawled_count}]: {str(e)[:50]}")
            except Exception as e:
                # æ•è·æ‰€æœ‰å…¶ä»–æœªå¤„ç†çš„å¼‚å¸¸
                status = "è‡´å‘½é”™è¯¯"
                self._update_status_stats("å…¶ä»–é”™è¯¯")
                self.logger.error(f"ğŸ’¥ å¤„ç†é¡µé¢æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯ [{self.crawled_count}]: {str(e)}", exc_info=True)

            if self.queue:
                self.logger.debug(f"âŒ› ç­‰å¾… {self.delay*1000:.0f}ms åç»§ç»­...")
                time.sleep(self.delay)

        # çˆ¬å–ç»“æŸæ—¶ä¿å­˜æœ€ç»ˆçš„CookieçŠ¶æ€
        self._save_current_cookies(self.start_url)
        
        # ä¿å­˜æ£€æµ‹åˆ°çš„PDFæ–‡ä»¶URL
        self._save_pdf_urls()
        
        # è¾“å‡ºåœæ­¢åŸå› 
        if not self.queue and self.crawled_count < self.max_pages:
            self.logger.info(f"ğŸ›‘ çˆ¬è™«åœæ­¢ï¼šé˜Ÿåˆ—å·²ç©ºï¼ˆå·²çˆ¬å– {self.crawled_count} é¡µï¼Œæœªè¾¾åˆ°æœ€å¤§é¡µé¢æ•° {self.max_pages}ï¼‰")
        elif self.crawled_count >= self.max_pages:
            self.logger.info(f"ğŸ›‘ çˆ¬è™«åœæ­¢ï¼šå·²è¾¾åˆ°æœ€å¤§é¡µé¢æ•° {self.max_pages}")

        # ä¿å­˜ç»“æœ
        if self.emails:
            self._save_results()
        else:
            self.logger.info("â„¹ï¸ æœªå‘ç°ä»»ä½•é‚®ç®±")
            
        self._save_blocked_urls()
        self._print_summary()

    def _save_results(self):
        """ä¿å­˜é‚®ç®±ç»“æœï¼Œç¡®ä¿æ¯ä¸ªé‚®ç®±åªå‡ºç°ä¸€æ¬¡ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # å†™å…¥ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºæ³¨é‡Š
                f.write(f"# é‚®ç®±æå–ç»“æœ - å…±å‘ç°{self.email_stats['found']}ä¸ªï¼Œå»é‡å{self.email_stats['unique']}ä¸ª\n")
                f.write(f"# å»é‡è§„åˆ™ï¼šå¤§å°å†™ä¸åŒè§†ä¸ºç›¸åŒé‚®ç®±\n")
                f.write(f"# çˆ¬å–å‚æ•°ï¼šè¶…æ—¶æ—¶é—´={self.timeout*1000:.0f}ms | çˆ¬å–æ·±åº¦={self.max_depth}\n")
                f.write(f"# ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ¥æºURLï¼š{self.start_url}\n")
                f.write("\n")
                
                # å†™å…¥å»é‡åçš„é‚®ç®±ï¼ˆä½¿ç”¨åŸå§‹å¤§å°å†™ï¼‰
                for email in self.emails.values():
                    f.write(f"{email}\n")
            
            self.logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(self.emails)} ä¸ªå”¯ä¸€é‚®ç®±åˆ° {self.output_file}ï¼ˆæ€»å‘ç°æ•°: {self.email_stats['found']}ï¼Œå»é‡{self.email_stats['duplicates']}ä¸ªï¼‰")
        except IOError as e:
            self.logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _save_blocked_urls(self):
        """ä¿å­˜è¢«æ‹¦æˆªçš„URL"""
        if not any(self.blocked_urls.values()):
            return
            
        try:
            with open(self.blocked_file, 'w', encoding='utf-8') as f:
                f.write("=== éœ€è¦ç™»å½•çš„URL ===\n")
                for url in sorted(self.blocked_urls["login"]):
                    f.write(f"{url}\n")
                
                f.write("\n=== åŒ…å«éªŒè¯ç çš„URL ===\n")
                for url in sorted(self.blocked_urls["captcha"]):
                    f.write(f"{url}\n")
            
            self.logger.info(f"ğŸ“‹ å·²ä¿å­˜ {len(self.blocked_urls['login'])+len(self.blocked_urls['captcha'])} ä¸ªè¢«æ‹¦æˆªURL")
        except IOError as e:
            self.logger.error(f"âŒ ä¿å­˜æ‹¦æˆªURLå¤±è´¥: {str(e)}")

    def _print_summary(self):
        """æ‰“å°çˆ¬å–æ€»ç»“ï¼ŒåŒ…å«è¶…æ—¶ç»Ÿè®¡"""
        self.logger.info("\n" + "="*50)
        self.logger.info("ğŸ“Š çˆ¬å–æ€»ç»“")
        self.logger.info(f"æ€»çˆ¬å–é¡µé¢: {self.crawled_count}")
        self.logger.info(f"é‚®ç®±ç»Ÿè®¡: å…±å‘ç° {self.email_stats['found']} ä¸ª | å»é‡å {self.email_stats['unique']} ä¸ª | è¿‡æ»¤é‡å¤ {self.email_stats['duplicates']} ä¸ª")
        self.logger.info(f"è¿æ¥ç»Ÿè®¡: æˆåŠŸ {sum(v for k, v in self.status_stats.items() if k not in ['è¶…æ—¶', 'å…¶ä»–é”™è¯¯', 'è¢«loginæ‹¦æˆª', 'è¢«captchaæ‹¦æˆª'])} æ¬¡ | è¶…æ—¶ {self.status_stats['è¶…æ—¶']} æ¬¡ | å…¶ä»–é”™è¯¯ {self.status_stats['å…¶ä»–é”™è¯¯']} æ¬¡")
        self.logger.info(f"æ¡†æ¶ç»Ÿè®¡: å‘ç° {self.frame_stats['found']} ä¸ª | å¤„ç† {self.frame_stats['processed']} ä¸ª | è¶…æ—¶ {self.frame_stats['è¶…æ—¶']} ä¸ª | å¤±è´¥ {self.frame_stats['failed']} ä¸ª")
        self.logger.info(f"æ£€æµ‹åˆ°å¹¶è·³è¿‡çš„PDFæ–‡ä»¶: {len(self.pdf_urls)} ä¸ª")
        self.logger.info(f"è¢«æ‹¦æˆªé¡µé¢: ç™»å½•é¡µé¢ {len(self.blocked_urls['login'])} ä¸ª | éªŒè¯ç é¡µé¢ {len(self.blocked_urls['captcha'])} ä¸ª")
        
        # Cookieä¿å­˜ç»Ÿè®¡
        self.logger.info(f"Cookieä¿å­˜: å…±ä¿å­˜ {len(self.saved_cookie_files)} ä¸ªæ–‡ä»¶åˆ° cookie æ–‡ä»¶å¤¹")
        if self.saved_cookie_files:
            self.logger.info(f"æœ€æ–°Cookieæ–‡ä»¶: {self.saved_cookie_files[-1]}")
        
        # é“¾æ¥ç»Ÿè®¡ï¼Œå¢åŠ PDFè¿‡æ»¤ç»Ÿè®¡
        self.logger.info(f"é“¾æ¥ç»Ÿè®¡: æå– {self.link_stats['extracted']} ä¸ª | æ–°å¢ {self.link_stats['added']} ä¸ª | é‡å¤ {self.link_stats['duplicates']} ä¸ª | è¿‡æ»¤ {self.link_stats['filtered']} ä¸ª | PDFè¿‡æ»¤ {self.link_stats['pdf']} ä¸ª")
        
        if self.handle_frames:
            self.logger.info(f"æ¡†æ¶å¤„ç†ç»Ÿè®¡: å‘ç° {self.frame_stats['found']} ä¸ª | å¤„ç† {self.frame_stats['processed']} ä¸ª | å¤±è´¥ {self.frame_stats['failed']} ä¸ª")
        
        self.logger.info(f"æœ€ç»ˆCookieæ•°é‡: {len(session.cookies)} ä¸ª")
        
        if self.speed_stats:
            avg_speed = sum(self.speed_stats) / len(self.speed_stats)
            self.logger.info(f"å¹³å‡å“åº”æ—¶é—´: {avg_speed:.3f}ç§’ | ä½¿ç”¨çš„è¶…æ—¶é˜ˆå€¼: {self.timeout}ç§’")
        
        self.logger.info("çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in self.status_stats.items():
            self.logger.info(f"  {status}: {count}æ¬¡")
        
        self.logger.info("æ·±åº¦åˆ†å¸ƒ:")
        for depth, count in sorted(self.depth_stats.items()):
            self.logger.info(f"  æ·±åº¦ {depth}: {count}é¡µ")
            
        self.logger.info("="*50)

def main():
    parser = argparse.ArgumentParser(
        description='é€šç”¨ç½‘ç»œçˆ¬è™«å·¥å…· - æ”¯æŒè‡ªå®šä¹‰è¶…æ—¶æ—¶é—´ï¼Œè‡ªåŠ¨å»é‡é‚®ç®±å¹¶è¿‡æ»¤PDFæ–‡ä»¶',
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # å¿…é€‰å‚æ•°
    parser.add_argument('url', help='ç›®æ ‡URLï¼ˆæ”¯æŒå¸¦GETå‚æ•°ï¼Œå¦‚ï¼šhttps://example.com/list?id=1&page=2ï¼‰')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('-d', '--delay', type=int, default=2000, 
                        help='çˆ¬å–é—´éš”(æ¯«ç§’)ï¼Œé»˜è®¤2000ms')
    parser.add_argument('-m', '--max-depth', type=int, default=3, 
                        help='æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œé»˜è®¤3å±‚')
    parser.add_argument('-p', '--max-pages', type=int, default=50, 
                        help='æœ€å¤§çˆ¬å–é¡µé¢æ•°ï¼Œé»˜è®¤50é¡µ')
    parser.add_argument('-v', '--verbose', action='store_true', 
                        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('--ignore-ssl', action='store_true', 
                        help='å¿½ç•¥SSLè¯ä¹¦éªŒè¯')
    parser.add_argument('--cookie', 
                        help='åˆå§‹Cookieæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cookie-mode', 
                        choices=['all', 'login-only', 'browsing-only'], 
                        default='all', 
                        help='Cookieå¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤allï¼‰')
    parser.add_argument('--cookie-frequency', type=int, default=10, 
                        help='Cookieä¿å­˜é¢‘ç‡ï¼ˆæ¯çˆ¬å–å¤šå°‘é¡µä¿å­˜ä¸€æ¬¡ï¼‰ï¼Œé»˜è®¤10é¡µ')
    parser.add_argument('--no-frames', action='store_false', dest='handle_frames',
                        help='ç¦ç”¨æ¡†æ¶(iframe/frame)å¤„ç†')
    parser.add_argument('--no-dynamic', action='store_false', dest='handle_dynamic_content',
                        help='ç¦ç”¨åŠ¨æ€å†…å®¹æå–')
    parser.add_argument('--cross-domain', action='store_true', dest='allow_cross_domain',
                        help='å…è®¸è·¨åŸŸçˆ¬å–ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰')
    # æ–°å¢è¶…æ—¶å‚æ•°
    parser.add_argument('-t', '--timeout', type=int, default=2000, 
                        help='è¶…æ—¶æ—¶é—´(æ¯«ç§’)ï¼Œé»˜è®¤2000msï¼ˆ2ç§’ï¼‰ï¼Œä¾‹å¦‚ -t 3000 è¡¨ç¤º3ç§’')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logger(args.verbose)
    
    # æ£€æŸ¥å¿…è¦çš„åº“
    try:
        import bs4
    except ImportError:
        logger.error("âŒ ç¼ºå°‘å¿…è¦çš„åº“ 'beautifulsoup4'ï¼Œè¯·å…ˆå®‰è£…: pip install beautifulsoup4")
        return
    
    if not has_colorlog:
        logger.warning("âš ï¸ æœªå®‰è£…colorlogï¼Œæ— æ³•æ˜¾ç¤ºå½©è‰²æ—¥å¿—ã€‚å¯è¿è¡Œ 'pip install colorlog' å®‰è£…ã€‚")
    
    try:
        crawler = GeneralCrawler(
            target_url=args.url,
            delay_ms=args.delay,
            max_depth=args.max_depth,
            max_pages=args.max_pages,
            ignore_ssl=args.ignore_ssl,
            cookie_file=args.cookie,
            cookie_mode=args.cookie_mode,
            save_cookies_frequency=args.cookie_frequency,
            logger=logger,
            allow_cross_domain=args.allow_cross_domain,
            timeout_ms=args.timeout  # ä¼ é€’è¶…æ—¶å‚æ•°
        )
        crawler.handle_frames = args.handle_frames
        crawler.handle_dynamic_content = args.handle_dynamic_content
        crawler.crawl()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜æ•°æ®ä¸­...")
        if hasattr(crawler, 'emails') and crawler.emails:
            crawler._save_results()
        if hasattr(crawler, '_save_blocked_urls'):
            crawler._save_blocked_urls()
        if hasattr(crawler, '_save_pdf_urls'):
            crawler._save_pdf_urls()  # ä¸­æ–­æ—¶ä¿å­˜PDF URL
        # ä¸­æ–­æ—¶ä¿å­˜å½“å‰Cookie
        if hasattr(crawler, '_save_current_cookies'):
            crawler._save_current_cookies(crawler.start_url)
        if hasattr(crawler, '_print_summary'):
            crawler._print_summary()
    except Exception as e:
        logger.error(f"âŒ çˆ¬è™«å¯åŠ¨å¤±è´¥: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()
    
